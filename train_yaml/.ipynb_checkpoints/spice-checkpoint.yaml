activation: silu
atom_filter: -1
attn_activation: silu
batch_size: 16
inference_batch_size: 16
test_size: 0.1
train_size: 0.8
val_size: 0.1
seed: 42
ngpus: [2]

wandb: true

model: equivariant-transformer
#change when using ET

diffusion_pretraining: false
emgp_pretraining: false
position_noise_scale: 0.
denoising_weight: 0.
denoising: false
denoising_multi: false
denoising_multinomial: false

linear_probing: false
finetune_output: false
load_pretrained: false
pretrained_model: null

toxicity_prediction: false
toxicity_weight: 1.0

energy_weight: 0.5
force_weight: 0.5

layernorm_on_vec: null

output_model: Scalar

#only needed for equivariant-transformer - change to Scalar + derivative true if training with conserved energies + forces; Forces learns forces directly

prior_model: null
#not needed in MACE

derivative: true
#not needed in MACE (currently only energy + force output in MACE)
max_z: 100
#change when using ET to highest nuclear charge (MACE: number of different nuclear charges)
num_atom_types: 22

cutoff_lower: 0.0
cutoff_upper: 10.0

dataset: SPICE
dataset_arg:
  version: 1.1.1
dataset_root: /workspace7/SPICE
#change for downstream pediction

#TensorNet
num_linears_tensor: 1
num_linears_scalar: 2
#ET
num_heads: 8

#General
num_layers: 3
embedding_dimension: 128
lr: 1.e-4
lr_metric: val_loss
lr_factor: 0.5
lr_min: 1.e-8
lr_patience: 15
early_stopping_patience: 50
lr_warmup_steps: 1000
lr_schedule: reduce_on_plateau
lr_cosine_length: 0
num_rbf: 64
trainable_rbf: false

standardize: false

test_interval: 10

distance_influence: both
ema_alpha_dy: 0.5
ema_alpha_y: 0.5

max_num_neighbors: 128
neighbor_embedding: true

num_epochs: 3000

num_nodes: 1
num_workers: 12
precision: 32
rbf_type: expnorm
redirect: false
reduce_op: add
save_interval: 10

splits: null
weight_decay: 0.0

